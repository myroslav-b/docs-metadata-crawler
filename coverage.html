
<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
		<title>app: Go Coverage Report</title>
		<style>
			body {
				background: black;
				color: rgb(80, 80, 80);
			}
			body, pre, #legend span {
				font-family: Menlo, monospace;
				font-weight: bold;
			}
			#topbar {
				background: black;
				position: fixed;
				top: 0; left: 0; right: 0;
				height: 42px;
				border-bottom: 1px solid rgb(80, 80, 80);
			}
			#content {
				margin-top: 50px;
			}
			#nav, #legend {
				float: left;
				margin-left: 10px;
			}
			#legend {
				margin-top: 12px;
			}
			#nav {
				margin-top: 10px;
			}
			#legend span {
				margin: 0 5px;
			}
			.cov0 { color: rgb(192, 0, 0) }
.cov1 { color: rgb(128, 128, 128) }
.cov2 { color: rgb(116, 140, 131) }
.cov3 { color: rgb(104, 152, 134) }
.cov4 { color: rgb(92, 164, 137) }
.cov5 { color: rgb(80, 176, 140) }
.cov6 { color: rgb(68, 188, 143) }
.cov7 { color: rgb(56, 200, 146) }
.cov8 { color: rgb(44, 212, 149) }
.cov9 { color: rgb(32, 224, 152) }
.cov10 { color: rgb(20, 236, 155) }

		</style>
	</head>
	<body>
		<div id="topbar">
			<div id="nav">
				<select id="files">
				
				<option value="file0">docscrawler/app/crawler.go (92.9%)</option>
				
				<option value="file1">docscrawler/app/engine.go (89.9%)</option>
				
				<option value="file2">docscrawler/app/main.go (0.0%)</option>
				
				<option value="file3">docscrawler/app/researchers/msox.go (42.2%)</option>
				
				<option value="file4">docscrawler/app/researchers/pdf.go (55.9%)</option>
				
				<option value="file5">docscrawler/app/researchers/researcher.go (83.3%)</option>
				
				<option value="file6">docscrawler/app/urlstorage.go (97.7%)</option>
				
				</select>
			</div>
			<div id="legend">
				<span>not tracked</span>
			
				<span class="cov0">not covered</span>
				<span class="cov8">covered</span>
			
			</div>
		</div>
		<div id="content">
		
		<pre class="file" id="file0" style="display: none">package main

import (
        "net/http"
        "net/url"
        "time"

        "golang.org/x/net/html"
)

// harv (harvest) extracts all links from the HTML document at the provided URL
// and adds them to the URL storage for further processing
func harv(baseUrl *url.URL, urlStorage *tUrlStorage) <span class="cov8" title="1">{
        // Initialize HTTP client with timeout
        client := &amp;http.Client{Timeout: 10 * time.Second}
        resp, err := client.Get(baseUrl.String())
        if err != nil </span><span class="cov8" title="1">{
                return
        }</span>
        <span class="cov8" title="1">defer resp.Body.Close()

        // Check if the response is successful
        if resp.StatusCode != http.StatusOK </span><span class="cov0" title="0">{
                return
        }</span>

        // Parse HTML content
        <span class="cov8" title="1">z := html.NewTokenizer(resp.Body)
        for </span><span class="cov8" title="1">{
                tt := z.Next()
                switch tt </span>{
                case html.ErrorToken:<span class="cov8" title="1">
                        return</span> // End of document
                case html.StartTagToken:<span class="cov8" title="1">
                        token := z.Token()

                        // Look for &lt;a&gt; tags
                        if token.Data == "a" </span><span class="cov8" title="1">{
                                for _, attr := range token.Attr </span><span class="cov8" title="1">{
                                        if attr.Key == "href" </span><span class="cov8" title="1">{
                                                link := attr.Val

                                                // Handle relative URLs
                                                url, err := resolveUrl(baseUrl.String(), link)
                                                if err != nil </span><span class="cov0" title="0">{
                                                        continue</span>
                                                }

                                                // Add link to results if it's new
                                                <span class="cov8" title="1">urlStorage.add(url)</span>
                                        }
                                }
                        }
                }
        }
}

// resolveUrl converts a relative URL to an absolute URL using the base URL
// Returns a parsed URL object or an error if parsing fails
func resolveUrl(baseStr string, href string) (*url.URL, error) <span class="cov8" title="1">{
        u, err := url.Parse(href)
        if err != nil </span><span class="cov8" title="1">{
                return nil, err
        }</span>
        <span class="cov8" title="1">base, err := url.Parse(baseStr)
        if err != nil </span><span class="cov8" title="1">{
                return nil, err
        }</span>
        <span class="cov8" title="1">return base.ResolveReference(u), nil</span>
}
</pre>
		
		<pre class="file" id="file1" style="display: none">package main

import (
        "bufio"
        "docscrawler/app/researchers"
        "errors"
        "fmt"
        "net/url"
        "os"
        "strings"
        "sync"
        "time"
)

// Time to wait between checks for available crawl threads
const crawlSleepTime = 5 * time.Second

// tEngine represents the main crawler engine
// Manages URL and document storages, processing parameters, and output configuration
type tEngine struct {
        url            *url.URL                          // Base URL to start crawling from
        urlStorage     *tUrlStorage                      // Storage for URLs discovered during crawling
        docStorage     map[string]researchers.Researcher // Storage for processed documents
        docTypes       []string                          // Document types/extensions to look for
        outputFileName string                            // Output file name (stdout if empty)
        paramax        int                               // Maximum number of parallel threads
        mutex          sync.Mutex                        // Mutex for thread-safe operations
}

// newEngine initializes a new crawler engine with the provided options
func newEngine(opts tOpts) (*tEngine, error) <span class="cov8" title="1">{

        engine := new(tEngine)
        engine.urlStorage = newUrlStorage()
        engine.docStorage = make(map[string]researchers.Researcher)
        engine.docTypes = make([]string, len(opts.Type))

        // Validate document types
        for i, st := range opts.Type </span><span class="cov8" title="1">{
                ok := researchers.Is(st)
                if !ok </span><span class="cov8" title="1">{
                        return nil, errors.New("unknown document format for analysis")
                }</span>
                <span class="cov8" title="1">engine.docTypes[i] = st</span>
        }

        <span class="cov8" title="1">engine.outputFileName = opts.Output

        engine.paramax = opts.Paramax

        // Parse and validate the starting URL
        var err error
        engine.url, err = url.ParseRequestURI(opts.Site)
        if err != nil </span><span class="cov8" title="1">{
                return engine, errors.New("invalid URL")
        }</span>

        <span class="cov8" title="1">return engine, nil</span>
}

// run executes the three main phases of the crawling process:
// 1. crawl - discover URLs
// 2. analyser - process documents
// 3. output - generate results
func (engine *tEngine) run() <span class="cov0" title="0">{
        engine.crawl()

        _ = engine.analyser()

        err := engine.output()
        if err != nil </span><span class="cov0" title="0">{
                fmt.Println(err.Error())
        }</span>

}

// crawl recursively discovers URLs starting from the base URL
// Uses a worker pool pattern with a guard channel to limit concurrent operations
func (engine *tEngine) crawl() <span class="cov8" title="1">{
        guard := make(chan bool, engine.paramax)
        defer close(guard)

        hostname := engine.url.Hostname()
        harv(engine.url, engine.urlStorage)

        for </span><span class="cov8" title="1">{
                urlBase, ok := engine.urlStorage.use()
                switch </span>{
                case !ok &amp;&amp; (len(guard) == 0):<span class="cov8" title="1">
                        // No more URLs to process and no active workers
                        return</span>
                case !ok &amp;&amp; (len(guard) &gt; 0):<span class="cov8" title="1">
                        // No URLs to process but workers are still active, wait
                        time.Sleep(crawlSleepTime)</span>
                case ok:<span class="cov8" title="1">
                        if isValidScheme(urlBase) &amp;&amp; (hostname == urlBase.Hostname()) </span><span class="cov8" title="1">{
                                guard &lt;- true
                                urlCopy := *urlBase
                                go func(u *url.URL) </span><span class="cov8" title="1">{
                                        harv(u, engine.urlStorage)
                                        &lt;-guard
                                }</span>(&amp;urlCopy)
                        }
                }
        }
}

// analyser processes discovered URLs looking for document files of specified types
// Uses a worker pool pattern with a guard channel to limit concurrent operations
func (engine *tEngine) analyser() error <span class="cov8" title="1">{

        guard := make(chan bool, engine.paramax)
        defer close(guard)

        var wg sync.WaitGroup

        for _, url := range engine.urlStorage.getAllUrls() </span><span class="cov8" title="1">{
                url := url
                guard &lt;- true
                wg.Add(1)
                go func() </span><span class="cov8" title="1">{
                        defer wg.Done()
                        engine.mutex.Lock()
                        defer engine.mutex.Unlock()

                        // Process URL if it has a matching document extension
                        for _, t := range engine.docTypes </span><span class="cov8" title="1">{
                                if strings.HasSuffix(url.String(), "."+t) </span><span class="cov8" title="1">{
                                        eng := researchers.New(t)
                                        err := eng.Do(url.String())
                                        if err == nil </span><span class="cov0" title="0">{
                                                engine.docStorage[url.String()] = eng
                                        }</span>
                                        <span class="cov8" title="1">break</span>
                                }
                        }
                        <span class="cov8" title="1">&lt;-guard</span>

                }()
        }

        <span class="cov8" title="1">wg.Wait()

        return nil</span>
}

// isValidScheme checks if the URL uses a supported protocol (http or https)
func isValidScheme(u *url.URL) bool <span class="cov8" title="1">{
        return u.Scheme == "http" || u.Scheme == "https"
}</span>

// output writes the analysis results to the specified output file or stdout
// Output is in JSON array format containing document metadata
func (engine *tEngine) output() error <span class="cov8" title="1">{
        //st := ""
        var out *os.File
        var err error

        // Determine output destination (file or stdout)
        if engine.outputFileName == "" </span><span class="cov8" title="1">{
                out = os.Stdout
        }</span> else<span class="cov8" title="1"> {
                out, err = os.Create(engine.outputFileName)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov8" title="1">defer out.Close()</span>
        }

        <span class="cov8" title="1">bufout := bufio.NewWriter(out)
        defer bufout.Flush()

        // Start JSON array
        bufout.WriteString("[")
        isFirst := true

        // Write each document's metadata as JSON object
        for _, url := range engine.urlStorage.getAllUrls() </span><span class="cov8" title="1">{
                rr, exists := engine.docStorage[url.String()]
                if exists </span><span class="cov8" title="1">{
                        if !isFirst </span><span class="cov0" title="0">{
                                bufout.WriteString(",")
                        }</span>
                        <span class="cov8" title="1">isFirst = false
                        _ = rr.OutJSON(bufout)</span>
                }
        }

        // Close JSON array
        <span class="cov8" title="1">bufout.WriteString("]")

        return nil</span>
}
</pre>
		
		<pre class="file" id="file2" style="display: none">package main

import (
        "log"
        "os"

        "github.com/jessevdk/go-flags"
)

// tOpts defines command line options for the document crawler
// Uses go-flags package for parsing and validation
type tOpts struct {
        Site    string   `short:"s" long:"site" required:"true" description:"site name"`
        Type    []string `short:"t" long:"type" choice:"pdf" choice:"docx" choice:"xlsx" choice:"pptx" description:"document type / file name extension (all if empty)"`
        Output  string   `short:"o" long:"output" default:"" description:"output stream, stdout if none"`
        Paramax int      `short:"p" long:"paramax" default:"100" description:"maximum number of parallel analysis threads"`
}

// main is the entry point of the application
// Parses command line arguments and starts the crawling engine
func main() <span class="cov0" title="0">{
        var opts tOpts

        // Initialize command line parser
        parser := flags.NewParser(&amp;opts, flags.Default)

        // Parse command line arguments
        if _, err := parser.Parse(); err != nil </span><span class="cov0" title="0">{
                os.Exit(1)
        }</span>

        // If no document types are specified, use all supported types
        <span class="cov0" title="0">typeOption := parser.FindOptionByLongName("type")
        allowDocTypes := typeOption.Choices
        if len(opts.Type) == 0 </span><span class="cov0" title="0">{
                opts.Type = allowDocTypes
        }</span>

        // Initialize and run the crawler engine
        <span class="cov0" title="0">engine, err := newEngine(opts)
        if err != nil </span><span class="cov0" title="0">{
                log.Fatalf("Engine initialization error: %v", err)
        }</span>

        <span class="cov0" title="0">engine.run()</span>
}
</pre>
		
		<pre class="file" id="file3" style="display: none">package researchers

import (
        "archive/zip"
        "encoding/json"
        "encoding/xml"
        "fmt"
        "io"
        "net/http"
        "os"
        "time"
)

// tCoreProperty represents core document properties from Office Open XML format
// Found in docProps/core.xml inside Office documents
type tCoreProperty struct {
        XMLName        xml.Name `xml:"coreProperties" json:"coreProperties,omitempty"`
        Title          string   `xml:"title" json:"title,omitempty"`
        Creator        string   `xml:"creator" json:"creator,omitempty"`
        LastModifiedBy string   `xml:"lastModifiedBy" json:"lastModifiedBy,omitempty"`
        Revision       string   `xml:"revision" json:"revision,omitempty"`
        Created        string   `xml:"created" json:"created,omitempty"`
        Modified       string   `xml:"modified" json:"modified,omitempty"`
        Language       string   `xml:"language" json:"language,omitempty"`
}

// tAppProperty represents application-specific properties from Office Open XML format
// Found in docProps/app.xml inside Office documents
type tAppProperty struct {
        XMLName     xml.Name `xml:"Properties" json:"properties,omitempty"`
        Application string   `xml:"Application" json:"application,omitempty"`
        DocSecurity string   `xml:"DocSecurity" json:"doc_security,omitempty"`
        Pages       string   `xml:"Pages" json:"pages,omitempty"`
        Words       string   `xml:"Words" json:"words,omitempty"`
        Characters  string   `xml:"Characters" json:"characters,omitempty"`
        Company     string   `xml:"Company" json:"company,omitempty"`
        Lines       string   `xml:"Lines" json:"lines,omitempty" `
        Paragraphs  string   `xml:"Paragraphs" json:"paragraphs,omitempty"`
        TotalTime   string   `xml:"TotalTime" json:"total_time,omitempty"`
        SharedDoc   string   `xml:"SharedDoc" json:"shared_doc,omitempty"`
        AppVersion  string   `xml:"AppVersion" json:"app_version,omitempty"`
}

// tMsox is a researcher for Microsoft Office Open XML files (docx, xlsx, pptx)
// Extracts metadata from the Office documents
type tMsox struct {
        docType      string
        Url          string `json:"url,omitempty"`
        CoreProperty tCoreProperty
        AppProperty  tAppProperty
}

// newMsox creates a new Microsoft Office document researcher
func newMsox() *tMsox <span class="cov8" title="1">{
        return new(tMsox)
}</span>

// OutJSON serializes the MSOX metadata to JSON and writes it to the provided writer
func (msox *tMsox) OutJSON(writer io.Writer) error <span class="cov8" title="1">{
        data, err := json.Marshal(msox)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">_, err = writer.Write(data)
        return err</span>
}

// Do performs the analysis of a Microsoft Office document at the given URL
// Downloads the file, extracts metadata from core.xml and app.xml, and stores it
func (msox *tMsox) Do(url string) error <span class="cov8" title="1">{
        msox.docType = "msox"
        msox.Url = url

        // Initialize HTTP client with timeout
        client := http.Client{
                Timeout: httpGetTimeout * time.Second,
        }
        resp, err := client.Get(url)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">defer resp.Body.Close()
        if resp.StatusCode != http.StatusOK </span><span class="cov8" title="1">{ // Check for 200 OK status
                // Can read response body for more detailed error if needed
                return fmt.Errorf("failed to download file: status code %d", resp.StatusCode)
        }</span>

        // Convert response body to a ReadSeeker for zip operations
        <span class="cov8" title="1">respReadSeeker, err := readCloserToReadSeekerFile(resp.Body)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Get temporary file name
        <span class="cov8" title="1">tmpFileName := respReadSeeker.Name()

        // Open ZIP archive (Office documents are ZIP archives)
        rZip, err := zip.OpenReader(tmpFileName)
        if err != nil </span><span class="cov8" title="1">{
                return err
        }</span>
        <span class="cov0" title="0">defer rZip.Close()

        // Process files inside the ZIP archive
        for _, fInZip := range rZip.File </span><span class="cov0" title="0">{
                switch fInZip.Name </span>{
                case "docProps/core.xml":<span class="cov0" title="0">
                        rc1, err := fInZip.Open()
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                        <span class="cov0" title="0">defer rc1.Close()
                        err = xml.NewDecoder(rc1).Decode(&amp;msox.CoreProperty)
                        //rc.Close()
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                case "docProps/app.xml":<span class="cov0" title="0">
                        rc2, err := fInZip.Open()
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                        <span class="cov0" title="0">defer rc2.Close()
                        err = xml.NewDecoder(rc2).Decode(&amp;msox.AppProperty)
                        //rc.Close()
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                default:<span class="cov0" title="0">
                        continue</span>
                }
        }

        // Clean up temporary file
        <span class="cov0" title="0">respReadSeeker.Close()
        err = os.Remove(tmpFileName)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov0" title="0">return nil</span>
}
</pre>
		
		<pre class="file" id="file4" style="display: none">package researchers

import (
        "encoding/json"
        "fmt"
        "io"
        "net/http"
        "os"
        "time"

        "github.com/pdfcpu/pdfcpu/pkg/api"
        "github.com/pdfcpu/pdfcpu/pkg/pdfcpu/model"
)

// tPdf is a researcher for PDF documents
// Extracts metadata from PDF files using pdfcpu library
type tPdf struct {
        docType      string
        Url          string `json:"url,omitempty"`
        FileName     string `json:"source,omitempty"`
        Version      string `json:"version,omitempty"`
        Title        string `json:"title,omitempty"`
        Author       string `json:"author,omitempty"`
        Subject      string `json:"subject,omitempty"`
        Producer     string `json:"producer,omitempty"`
        Creator      string `json:"creator,omitempty"`
        CreationDate string `json:"creation_date,omitempty"`
        ModDate      string `json:"mod_date,omitempty"`
}

// newPdf creates a new PDF document researcher
func newPdf() *tPdf <span class="cov8" title="1">{
        return new(tPdf)
}</span>

// OutJSON serializes the PDF metadata to JSON and writes it to the provided writer
func (pdf *tPdf) OutJSON(writer io.Writer) error <span class="cov8" title="1">{
        data, err := json.Marshal(pdf)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">_, err = writer.Write(data)
        return err</span>
}

// Do performs the analysis of a PDF document at the given URL
// Downloads the file, extracts metadata, and stores it
func (pdf *tPdf) Do(url string) error <span class="cov8" title="1">{
        pdf.docType = "pdf"
        pdf.Url = url

        // Initialize HTTP client with timeout
        client := http.Client{
                Timeout: httpGetTimeout * time.Second,
        }
        resp, err := client.Get(url)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">defer resp.Body.Close()
        if resp.StatusCode != http.StatusOK </span><span class="cov8" title="1">{ // Check for 200 OK status
                // Can read response body for more detailed error if needed
                return fmt.Errorf("failed to download file: status code %d", resp.StatusCode)
        }</span>

        // Convert response body to a ReadSeeker for PDF operations
        <span class="cov8" title="1">respReadSeeker, err := readCloserToReadSeekerFile(resp.Body)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Get PDF information using pdfcpu library
        <span class="cov8" title="1">tmpFileName := respReadSeeker.Name()
        info, err := api.PDFInfo(respReadSeeker, tmpFileName, nil, model.NewDefaultConfiguration())
        if err != nil </span><span class="cov8" title="1">{
                return err
        }</span>

        // Clean up temporary file
        <span class="cov0" title="0">respReadSeeker.Close()
        err = os.Remove(tmpFileName)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Store extracted metadata
        <span class="cov0" title="0">pdf.Title = info.Title
        pdf.Author = info.Author
        pdf.Subject = info.Subject
        pdf.Creator = info.Creator
        pdf.Producer = info.Producer
        pdf.CreationDate = info.CreationDate
        pdf.ModDate = info.ModificationDate

        return nil</span>
}
</pre>
		
		<pre class="file" id="file5" style="display: none">package researchers

import (
        "fmt"
        "io"
        "os"
)

// Constants for HTTP timeout and file size limits
const (
        httpGetTimeout = 30                // HTTP request timeout in seconds
        maxFileSize    = 100 * 1024 * 1024 // Maximum file size (100MB)
)

// Map of supported file types to their researcher factory functions
var allFileTypes = map[string]func() Researcher{
        "pdf":  func() Researcher <span class="cov8" title="1">{ return newPdf() }</span>,
        "docx": func() Researcher <span class="cov8" title="1">{ return newMsox() }</span>,
        "xlsx": func() Researcher <span class="cov8" title="1">{ return newMsox() }</span>,
        "pptx": func() Researcher <span class="cov8" title="1">{ return newMsox() }</span>,
}

// Is checks if the specified file type/extension is supported
func Is(st string) bool <span class="cov8" title="1">{
        _, exist := allFileTypes[st]
        return exist
}</span>

// New creates a new researcher instance for the specified file type
func New(st string) Researcher <span class="cov8" title="1">{
        f := allFileTypes[st]
        return f()
}</span>

// Researcher interface defines the common operations for document metadata extraction
// Implementations should be able to analyze documents and output results as JSON
type Researcher interface {
        OutJSON(writer io.Writer) error // Write metadata as JSON to the provided writer
        Do(url string) error            // Process document at the given URL
}

// readCloserToReadSeekerFile converts an io.ReadCloser to an os.File (which implements io.ReadSeeker)
// This is necessary because many document processing libraries require io.ReadSeeker functionality
// The function creates a temporary file, copies content from the reader, and returns the file
// Caller is responsible for closing and removing the temporary file when finished
func readCloserToReadSeekerFile(rc io.ReadCloser) (*os.File, error) <span class="cov8" title="1">{

        // Create a temporary file
        tmpFile, err := os.CreateTemp("", "readseeker-*")
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // Copy data with size limit
        <span class="cov8" title="1">limitedReader := &amp;io.LimitedReader{R: rc, N: maxFileSize}
        _, err = io.Copy(tmpFile, limitedReader)
        if err != nil </span><span class="cov8" title="1">{
                tmpFileName := tmpFile.Name()
                tmpFile.Close()
                os.Remove(tmpFileName)
                return nil, err
        }</span>

        // Check if size limit was reached (indicates file is too large)
        <span class="cov8" title="1">if limitedReader.N == 0 </span><span class="cov8" title="1">{
                tmpFileName := tmpFile.Name()
                tmpFile.Close()
                os.Remove(tmpFileName)
                return nil, fmt.Errorf("file exceeds maximum allowed size of %d bytes", maxFileSize)
        }</span>

        // Seek to beginning of file
        <span class="cov8" title="1">_, err = tmpFile.Seek(0, io.SeekStart)
        if err != nil </span><span class="cov0" title="0">{
                tmpFileName := tmpFile.Name()
                tmpFile.Close()
                os.Remove(tmpFileName)
                return nil, err
        }</span>

        <span class="cov8" title="1">return tmpFile, nil</span>
}
</pre>
		
		<pre class="file" id="file6" style="display: none">package main

import (
        "net/url"
        "sync"
)

// tUrlStorage manages URL collection, status tracking, and processing queue
// with thread-safe operations using RWMutex for concurrent access control
type tUrlStorage struct {
        mu         sync.RWMutex        // RWMutex for concurrent access control
        urlStatus  map[string]bool     // URL status map (true = used/processed)
        urlObjects map[string]*url.URL // Map of string keys to URL objects
        queue      []string            // Queue of URLs to be processed
}

// newUrlStorage creates and initializes a new URL storage instance
func newUrlStorage() *tUrlStorage <span class="cov8" title="1">{
        return &amp;tUrlStorage{
                urlStatus:  make(map[string]bool),
                urlObjects: make(map[string]*url.URL),
                queue:      make([]string, 0, 100),
        }
}</span>

// Add adds a new URL to the storage if it doesn't already exist
// Returns true if URL was added, false if it already existed or is nil
func (us *tUrlStorage) add(u *url.URL) bool <span class="cov8" title="1">{
        if u == nil </span><span class="cov8" title="1">{
                return false
        }</span>

        <span class="cov8" title="1">us.mu.Lock()
        defer us.mu.Unlock()

        key := u.String()

        // Check if URL already exists
        if _, exists := us.urlStatus[key]; exists </span><span class="cov8" title="1">{
                return false
        }</span>

        // Store a copy of the URL
        <span class="cov8" title="1">urlCopy := *u // Create a copy of the URL structure
        us.urlObjects[key] = &amp;urlCopy
        us.urlStatus[key] = false // false = unused
        us.queue = append(us.queue, key)

        return true</span>
}

// Use returns an unused URL and marks it as used
// Returns the URL and true if successful, nil and false if no unused URLs exist
func (us *tUrlStorage) use() (*url.URL, bool) <span class="cov8" title="1">{
        us.mu.Lock()
        defer us.mu.Unlock()

        // Find an unused URL in the queue
        for i := 0; i &lt; len(us.queue); i++ </span><span class="cov8" title="1">{
                key := us.queue[i]

                if !us.urlStatus[key] </span><span class="cov8" title="1">{
                        // Mark as used
                        us.urlStatus[key] = true

                        // Remove from queue (fast removal without preserving order)
                        us.queue[i] = us.queue[len(us.queue)-1]
                        us.queue = us.queue[:len(us.queue)-1]

                        return us.urlObjects[key], true
                }</span>
        }

        <span class="cov8" title="1">return nil, false</span>
}

// GetAllURLs returns all URLs stored in the storage
func (us *tUrlStorage) getAllUrls() []*url.URL <span class="cov8" title="1">{
        us.mu.RLock()
        defer us.mu.RUnlock()

        result := make([]*url.URL, 0, len(us.urlObjects))

        for _, urlObj := range us.urlObjects </span><span class="cov8" title="1">{
                // Important: return the stored pointers, not creating new ones
                result = append(result, urlObj)
        }</span>

        <span class="cov8" title="1">return result</span>
}

// Check verifies if a URL exists in storage and whether it's already used
// Returns (exists, used) as booleans
func (us *tUrlStorage) check(u *url.URL) (exists bool, used bool) <span class="cov8" title="1">{
        if u == nil </span><span class="cov0" title="0">{
                return false, false
        }</span>

        <span class="cov8" title="1">us.mu.RLock()
        defer us.mu.RUnlock()

        key := u.String()
        used, exists = us.urlStatus[key]
        return exists, used</span>
}

// Count returns the total number of URLs in storage and how many are used
func (us *tUrlStorage) count() (total int, used int) <span class="cov8" title="1">{
        us.mu.RLock()
        defer us.mu.RUnlock()

        total = len(us.urlStatus)

        for _, isUsed := range us.urlStatus </span><span class="cov8" title="1">{
                if isUsed </span><span class="cov8" title="1">{
                        used++
                }</span>
        }

        <span class="cov8" title="1">return total, used</span>
}
</pre>
		
		</div>
	</body>
	<script>
	(function() {
		var files = document.getElementById('files');
		var visible;
		files.addEventListener('change', onChange, false);
		function select(part) {
			if (visible)
				visible.style.display = 'none';
			visible = document.getElementById(part);
			if (!visible)
				return;
			files.value = part;
			visible.style.display = 'block';
			location.hash = part;
		}
		function onChange() {
			select(files.value);
			window.scrollTo(0, 0);
		}
		if (location.hash != "") {
			select(location.hash.substr(1));
		}
		if (!visible) {
			select("file0");
		}
	})();
	</script>
</html>
